{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ab199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set up the Chrome driver (you need to have Chrome installed)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# List of URLs to process\n",
    "urls = [\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S026382310800013X?via%3Dihub\",\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S0141029617304996#t0025\",\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S026382311730215X#t0010\",\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S026382311930196X#tbl2\",\n",
    "    #\"https://www.sciencedirect.com/science/article/pii/S0263823114003139#t0015\"\n",
    "]\n",
    "\n",
    "# Directory to save the scraped files\n",
    "directory = r\"C:\\Users\\23109\"\n",
    "\n",
    "# Prefix to search for in file names\n",
    "file_name_prefix = \"Experiments on cold-formed steel columns with holes\"\n",
    "\n",
    "# Create the \"scrape\" folder if it doesn't exist\n",
    "folder_path = \"scrape\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Iterate over the URLs\n",
    "cnt = 0\n",
    "for url in urls:\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "    cnt += 1\n",
    "    if cnt==1:\n",
    "        # Wait for 20 seconds for manual login\n",
    "        time.sleep(40)\n",
    "    time.sleep(5)\n",
    "    # Check if the button exists on the page\n",
    "    button = driver.find_elements(\"xpath\", \"//span[contains(@class, 'button-link-text') and contains(text(), 'Show more figures')]\")\n",
    "\n",
    "    if button:\n",
    "        # Click the button to load more figures\n",
    "        button[0].click()\n",
    "        time.sleep(5)  # Wait for the figures to load\n",
    "    else:\n",
    "        print(\"The 'Show more figures' button was not found.\")\n",
    "\n",
    "    # Retrieve the HTML content\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Find all possible HTMLs with a similar format\n",
    "    possible_htmls = re.findall(r'<div class=\"tables[^>]*>.*?</div>', html_content, flags=re.DOTALL)\n",
    "\n",
    "    # Extract displayed text and save the tables as text and original HTML files\n",
    "    for i, html_snippet in enumerate(possible_htmls):\n",
    "        modified_html = html_snippet.replace('<span class=\"label\">', '<span class=\"label\" style=\"color: black;\">')\n",
    "        table_html = f\"<div style='color:black;'>{modified_html}</div>\"\n",
    "        display(HTML(table_html))\n",
    "\n",
    "        # Save the original HTML as a text file\n",
    "        with open(f\"table_{i+1}_original.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(table_html)\n",
    "\n",
    "        print(f\"Table {i+1} saved as text and original HTML files.\")\n",
    "\n",
    "    # Download and save the images\n",
    "    figure_links = re.findall(r'<img alt=\".*?\" class=\"u-display-block\" height=\".*?\" src=\"(.*?)\" width=\".*?\">', html_content)\n",
    "    print('Number of figures:', len(figure_links))\n",
    "    figure_links = [url.replace('.sml', '.jpg') for url in figure_links]\n",
    "\n",
    "    for i, image_url in enumerate(figure_links):\n",
    "        try:\n",
    "            # Send a GET request to the image URL\n",
    "            response = requests.get(image_url)\n",
    "\n",
    "            # Check if the request was successful (status code 200)\n",
    "            if response.status_code == 200:\n",
    "                # Get the file name from the URL\n",
    "                file_name = f\"{file_name_prefix}_{i+1}.jpg\"\n",
    "\n",
    "                # Save the image in the current working directory\n",
    "                with open(file_name, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "\n",
    "                print(f\"Image {i+1} downloaded successfully!\")\n",
    "            else:\n",
    "                print(f\"Failed to download Image {i+1}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading Image {i+1}: {e}\")\n",
    "\n",
    "    # Move the files to the \"scrape\" folder\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(file_name_prefix):\n",
    "            # Get the full path of the file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Move the file to the \"scrape\" folder\n",
    "            destination_path = os.path.join(folder_path, filename)\n",
    "            shutil.move(file_path, destination_path)\n",
    "\n",
    "            print(f\"File '{filename}' moved to 'scrape' folder.\")\n",
    "\n",
    "            \n",
    "    import re\n",
    "    from IPython.display import display, HTML\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "\n",
    "    # Find all possible HTMLs with a similar format\n",
    "    # Create an empty DataFrame to store the merged tables\n",
    "    merged_table = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate over the tables\n",
    "    first_column_names = []\n",
    "\n",
    "    # Iterate over the tables\n",
    "    for html_snippet in possible_htmls:\n",
    "        modified_html = html_snippet.replace('<span class=\"label\">', '<span class=\"label\" style=\"color: black;\">')\n",
    "        table_html = f\"<div style='color:black;'>{modified_html}</div>\"\n",
    "\n",
    "        # Convert the HTML table to a DataFrame using BeautifulSoup\n",
    "        soup = BeautifulSoup(table_html, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        headers = table.findAll('th')\n",
    "\n",
    "        # Get the name of the first column\n",
    "        if headers:\n",
    "            first_column_name = headers[0].text.strip()\n",
    "            first_column_names.append(first_column_name)\n",
    "\n",
    "    # Find the most common string in the first column names\n",
    "    most_common_column_name = max(set(first_column_names), key=first_column_names.count)\n",
    "\n",
    "\n",
    "    # Iterate over the tables\n",
    "    for i, html_snippet in enumerate(possible_htmls):\n",
    "        modified_html = html_snippet.replace('<span class=\"label\">', '<span class=\"label\" style=\"color: black;\">')\n",
    "        table_html = f\"<div style='color:black;'>{modified_html}</div>\"\n",
    "\n",
    "        # Convert the HTML table to a DataFrame using BeautifulSoup\n",
    "        soup = BeautifulSoup(table_html, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        df = pd.read_html(str(table))[0]\n",
    "\n",
    "        # Check if the first column contains the most_common_column_name\n",
    "        #print(df.columns)\n",
    "        if not any(most_common_column_name.lower() in str(col).lower() for col in df.columns):\n",
    "            # Skip the table and report the issue\n",
    "            print(f\"Skipping Table {i+1} due to missing first column '{most_common_column_name}'\")\n",
    "            continue\n",
    "\n",
    "        # Check if the DataFrame contains a column with the name containing \"Specimen\"\n",
    "        specimen_columns = [col for col in df.columns if most_common_column_name in col]\n",
    "        if len(specimen_columns) > 0:\n",
    "            # Keep only one column with the name containing \"Specimen\"\n",
    "            df = df.drop(specimen_columns[1:], axis=1)\n",
    "\n",
    "        # Add a suffix to the columns to avoid overlap\n",
    "        suffix = f\"_{i+1}\"\n",
    "        df = df.add_suffix(suffix)\n",
    "\n",
    "        # Merge the current table with the existing merged table\n",
    "        if merged_table.empty:\n",
    "            merged_table = df\n",
    "        else:\n",
    "            print(merged_table.shape)\n",
    "            print(df.shape)\n",
    "            #merged_table = merged_table.join(df, how='outer')\n",
    "            merged_table.columns = merged_table.columns.get_level_values(0)\n",
    "            df.columns = df.columns.get_level_values(0)\n",
    "            merged_table = pd.merge(merged_table, df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "        # Save the original HTML as a text file\n",
    "        with open(f\"table_{i+1}_original.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(table_html)\n",
    "\n",
    "        print(f\"Table {i+1} saved as text and original HTML files.\")\n",
    "\n",
    "    # Get the column names dynamically that contain the most_common_column_name\n",
    "    columns_to_merge = [col for col in merged_table.columns if most_common_column_name in col]\n",
    "\n",
    "    # Merge the columns together and change the column name dynamically\n",
    "    merged_table[most_common_column_name] = merged_table[columns_to_merge].fillna('').apply(lambda row: ''.join(row), axis=1)\n",
    "\n",
    "    # Drop the redundant columns except for the merged column\n",
    "    merged_table.drop(columns=columns_to_merge[1:], inplace=True)\n",
    "\n",
    "    # Rename the column to \"Specimen\"\n",
    "    merged_table1 = merged_table.rename(columns={most_common_column_name: most_common_column_name})\n",
    "    '''\n",
    "    # Iterate over each row of the DataFrame\n",
    "    for index, row in merged_table.iterrows():\n",
    "        mark_element = row[most_common_column_name]  # Get the element to mark from the 'Specimen' column\n",
    "\n",
    "        # Check if the mark element is not NaN and is not of float type\n",
    "        if not pd.isna(mark_element) and not isinstance(mark_element, float):\n",
    "            # Iterate over each column of the row\n",
    "            for column in merged_table.columns:\n",
    "                # Check if the column contains the marked element (excluding the 'Specimen' column itself)\n",
    "                if column != most_common_column_name and mark_element in np.array(row[column]):\n",
    "                    merged_table = merged_table.drop(columns=column)  # Drop the column\n",
    "    '''\n",
    "    # Print the modified DataFrame\n",
    "    print('')\n",
    "    print('')\n",
    "    print('Merged Table of Article ', cnt)\n",
    "    display(merged_table1)\n",
    "    print('')\n",
    "    print('')\n",
    "    filename = f\"Merged Table of article {cnt}.csv\"\n",
    "    merged_table.to_csv(filename, index=False)\n",
    "    print(f'file{cnt}_saved!')\n",
    "    print('')\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
